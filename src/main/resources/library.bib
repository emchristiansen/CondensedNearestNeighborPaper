@MISC{elkan-knn,
title = {Nearest Neighbor Classification},
note = {\url{http://www-cse.ucsd.edu/users/elkan/250B/nearestn.pdf} (accessed on 7 December 2008)},
author = {Charles Elkan}
}

@article{Cover:Hart:1967,
	author = {Cover, Thomas   and Hart, Peter  E. },
	citeulike-article-id = {3245907},
	journal = {{IEEE} Transactions on Information Theory},
	keywords = {file-import-08-09-12},
	number = {1},
	pages = {21--27},
	posted-at = {2008-09-12 14:30:35},
	priority = {2},
	title = {Nearest neighbor pattern classification},
	volume = {13},
	year = {1967}
}

@ARTICLE{HartCNN,
title={The condensed nearest neighbor rule (Corresp.)},
author={ Hart, P.},
journal={Information Theory, IEEE Transactions on},
year={1968},
month={May},
volume={14},
number={3},
pages={ 515-516},
keywords={null Pattern classification},
doi={},
ISSN={0018-9448}, }

@article{Devroye,
	author = {Luc Devroye},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	pages = {75--78},
	title = {On the inequality of {Cover} and {Hart} in nearest neighbor discrimination},
	volume = {3},
	year = {1981}
}

@inproceedings{Wilfong,
 author = {Gordon Wilfong},
 title = {Nearest neighbor problems},
 booktitle = {SCG '91: Proceedings of the seventh annual symposium on Computational geometry},
 year = {1991},
 isbn = {0-89791-426-0},
 pages = {224--233},
 location = {North Conway, New Hampshire, United States},
 doi = {http://doi.acm.org/10.1145/109648.109673},
 publisher = {ACM},
 address = {New York, NY, USA},
 }

@INPROCEEDINGS{Toussaint,
    author = {Godfried Toussaint},
    title = {Proximity graphs for nearest neighbor decision rules: recent progress},
    booktitle = {Proceedings of the 34th Symposium on the INTERFACE},
    year = {2002},
    pages = {17--20}
}

@INPROCEEDINGS{Wilson,
    author = {D. Randall Wilson and Tony R. Martinez},
    title = {Reduction Techniques for Instance-Based Learning Algorithms},
    booktitle = {Machine Learning},
    year = {2000},
    pages = {257--286}
}

@inproceedings{Angiulli,
 author = {Fabrizio Angiulli},
 title = {Fast condensed nearest neighbor rule},
 booktitle = {ICML '05: Proceedings of the 22nd international conference on Machine learning},
 year = {2005},
 isbn = {1-59593-180-5},
 pages = {25--32},
 location = {Bonn, Germany},
 doi = {http://doi.acm.org/10.1145/1102351.1102355},
 publisher = {ACM},
 address = {New York, NY, USA},
 }

@book{bishop-1995,
	abstract = {This book provides a solid statistical foundation for neural networks from a pattern recognition perspective. The focus is on the types of neural nets that are most widely used in practical applications, such as the multi-layer perceptron and radial basis function networks. Rather than trying to cover many different types of neural networks, Bishop thoroughly covers topics such as density estimation, error functions, parameter optimization algorithms, data pre-processing, and Bayesian methods. All topics are organized well and all mathematical foundations are explained before being applied to neural networks. The text is suitable for a graduate or advanced undergraduate level course on neural networks or for practitioners interested in applying neural networks to real-world problems. The reader is assumed to have the level of math knowledge necessary for an undergraduate science degree. This is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. Also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and Bayesian techniques and their applications. Designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition.},
	author = {Bishop, Christopher  M. },
	citeulike-article-id = {308856},
	howpublished = {Paperback},
	isbn = {0198538642},
	keywords = {machine\_learning},
	month = {November},
	posted-at = {2007-11-01 19:29:18},
	priority = {0},
	publisher = {Oxford University Press},
	title = {Neural Networks for Pattern Recognition},
	url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0198538642},
	year = {1995}
}

@book{bishop-2006,
	abstract = {{The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.}},
	author = {Bishop, Christopher  M. },
	citeulike-article-id = {873540},
	howpublished = {Hardcover},
	isbn = {0387310738},
	keywords = {book, machine\_learning, pattern\_classification},
	month = {August},
	posted-at = {2007-10-28 16:57:29},
	priority = {0},
	publisher = {Springer},
	title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
	year = {2006},
}

@inproceedings{CollinsPerceptron,
 author = {Collins,, Michael},
 title = {Discriminative training methods for hidden {Markov} models: theory and experiments with perceptron algorithms},
 booktitle = {EMNLP '02: Proceedings of the ACL-02 conference on empirical methods in natural language processing},
 year = {2002},
 pages = {1--8},
 doi = {http://dx.doi.org/10.3115/1118693.1118694},
 publisher = {Association for Computational Linguistics},
 address = {Morristown, NJ, USA},
 }

	
	